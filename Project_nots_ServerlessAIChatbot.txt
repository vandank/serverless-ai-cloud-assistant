AWS Serverless AI-Powered-Chatbot using Amazon Bedrock
[Followed Procedure for this project : Security -> Cost -> Observability -> Intelligence]

-----------------
Architecture:
-----------------
Client (curl/Postman)
   |
API Gateway (HTTP API)
   |
Lambda (Python)
   |
Amazon Bedrock (LLM)

==========================================================
STEP 0: Cost Guardrails
Create a budget before starting the project. 
 - Go to AWS Console > billing >  budget and create a budget (Zero-Spend) keeping everything default. This will set a threshold o f0.01$ and emial me whenever my account detects 1 cent used. 
 We will have TWO Budgets:
 1. Zero-Spend Budget:
	Purpose: Immediate Alarm. Triggers at $0.01.
 2. Monthly Cost Budget($2-$3)
    Purpose: Safety net. Allows me to use Bedrock + Lambda without panic. Gives me time before real money is being spent
....................................
STEP 1: AWS CLI Sanity Check
The goal to do this is to ensure the AWS CLI works correctly and I'm deploying it to the right AWS Account

Run commnd: >> aws sts get-caller-identity
(Sometiems this may raise an issue that " 'more' is not recognized as an internal or external command, operable program or batch file."
 This is AWS CLI + Windows powershell problem. AWS CLI v2 auto uses a pager. I disabled this in the powershell by using $env:AWS_PAGER="" 
 THis didn't work well so i forcibly used:
 aws sts get-caller-identity --no-cli-pager
 )
......................................
STEP 2: AWS Serverless Application Module (SAM) CLI 
This will be used as our Infrastructure as Code, easy deployment, and easy cleanup (Cost Control)
.......................................
STEP 3: Enable Amazon Bedrock 
Enable Amazon Bedrock in the same region as cli us-east-1. For this project using Claude Haiku 4.5
.......................................
STEP 4: Create the Serverless Project Skeleton (AWS SAM)
This step will set up:
- API Gateway
- Lambda
- Deployment wiring
- Clean roolback & cleanup

Test Claude Haiku 4.5 in playground on console then do "sam init" in the vscode terminal. This will create a template for us where we will have the basic hello world template having files template.yaml, a folder called hello_world and app.py inside this folder
.......................................
STEP 5: Write the Lambda Logic (Bedrock -> Claude Haiku)
Written the logic in app.py where it invokes the claude model with prompt max_token limit upto 200 (prompt length limited to 500 chars).
Now, after writing the code it is necessary to follow these commands and to test our build locally. (Local Test with SAM)
	1. >>> sam build
	2. After the build says Build Success, do this:
	   >>> sam local start-api
*****************************************************************************************
Here, I faced several issues while doing sam local start-api.
First, I got this error:
=> Error: Running AWS SAM projects locally requires a container runtime. Do you have Docker installed and running?
It means that sam local start-api requires a Docker bcz SAM runs your lambda locally inside a Docker container that micmics AWS Lambda

NOTE: SAM needs Docker! 
Why?
AWS Lambda:
 - Runs on Amazon Linux
 - Has a very specific runtime environment

My Windows Machine is not Lambda environment

So, What SAM does is:
My Code -> Docker container -> Lambda-like runtime environment
*****************************************************************************************
After facing the above issues, sam local start-api worked successfully and I got the SUCCESS and the mounted message which displayed where my API lives like this:
Mounting HelloWorldFunction at http://127.0.0.1:3000/hello

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NOTE: Some of the change had to be done in the template:
In my HelloWorldFunction properties, i changed:
	- Runtime: python3.11 from python3.14
	- Added MemorySize property
	- Added Policies to allow bedrock invokeModel
	- Changed the Method to POST from GET
	- Increased the timeout from 3 to 10
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TO test the above build locally, this is the command:
Invoke-RestMethod `
  -Uri "http://127.0.0.1:3000/hello" `
  -Method POST `
  -Headers @{ "Content-Type" = "application/json" } `
  -Body (@{prompt="Explain AWS Lambda in one sentence."} | ConvertTo-Json)
	
......................................
STEP 6: Deploy to AWS 

Final Sanity check:
>> sam validate. If this succeds, then proceed ahead.
Again, even though I built locally earlier, it is recommended to always rebuild before deploying.

After re-buidling, I did:
>> sam build --guided

After answering all the prompts in the guided build, got a SUCCESS OUTPUT which gave me my HelloWorldAPI -- this is the live API

=======================================================================
After this changing the Architecture:
1. Maintaining API Key + Throttling 
-- This step will limit API to only userswith the API key. Anonymous users will not be able to call my API.
-- API Gateway enforces:
	- Rate Limit
	- Daily Quota
-- Bedrock cost is protected at the gateway level (before lambda runs)
--------------------------------------
Architecture:						  |
	Client (API Key)				  |
		  |							  |
	API Gateway (throttling + quota)  |
		  |							  |
		Lambda						  |
		  |							  |
	Amazon Bedrock					  |
---------------------------------------	
Bad traffic will be stopped before Lambda or Bedrock runs
Now,
1. I added HelloWoldAPIUsagePlan for Throttling + Quota 
This makes sure:
- Rate/Limit is set to 5 requests/sec -> stops flood requests
- Burst 10 -> allows short spikes
- 100 requests per day

2. Then created the API key resource
3. Attached the API key to the usage plan i.e., link the key and the plan
   Thus so far, the 'template.yaml' has:
	- API key required on /hello
	- Usage plan with throttling + quota
	- API key resource
	- Usage plan key binding 
   API Key is protected!
   
Now, test it locally with using API as well as without providing the API key. The latter will fail as it is configured to provide API! This latter case will give a 403 - Forbidden Error

TEST with API:
Invoke-RestMethod `
  -Uri "https://YOUR-API/Prod/hello/" `
  -Method POST `
  -Headers @{
      "Content-Type" = "application/json"
      "x-api-key"    = "YOUR-API"
  } `
  -Body (@{
      prompt = "Explain AWS Lambda in one sentence."
  } | ConvertTo-Json)

............................................................
STEP 7: Token & Prompt Cost hardening 
Right now the API is protected but each request can still burn tokens. Need to make sure that:
- Every request is cheap by default
- The model is guided to answer concisely
- I don't accidently pay for verbosity
	
	STEP 7.1: Add a System Instruction.
	This step will contrain the behavior of the LLM model to answer in limits without giving any unnecessary details. Also, this will reduce the output tokens dramatically. (This is #1 LLM cost control technique)
	
	STEP 7.2
	Added a Dynamic token cap since not all prompt deserves 100 tokens. 
	
	STEP 7.3
	if the prompt contains words like "hi", "hey", "hello", "what's up", "How are you?" simply return response:  "Hello! Ask me a technical question." THis will exit early for trivial prompts.
	
	Redeploy and test 
	
***Learning: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
In Step 7.1 when I added the System level instruction in the messages, it gave me an error while testing my API call:
Invoke-RestMethod : {"error": "An error occurred (ValidationException) when calling the InvokeModel operation: messages: Unexpected 
role \"system\". The Messages API accepts a top-level `system` parameter, not \"system\" as an input message role."}

This is bcz I had written the piece of code for system in messages section which is a OpenAI Style and not what Claude expects. 
Claude expects:
- system -> top-level field
- messages -> only user/assistant roles

FIX: Add the system part right after anthropic_version in the payload!
***^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

==========================================================================
PHASE-1 ADDING OBSERVABILITY
==========================================================================
..............................................................................
STEP: Lightweight Observability
Goal of this step: Need to understand what my AI system is doing without adding cost or complexity
For every request, I need to know: 
	- Request ID
	- Prompt length
	- Token Cap used
	- Latency(ms)
	- Whether Bedrock was called or short circuited
(This is exactly what real AI teams monitor)

	STEP 8.1: Add structured Logging:
	- Adding logger at the start of the code in 'app.py'
	- Start the track request lifecycle in the lambda_handler
	- Also, log the early exit for trivial prompts
	- Log before calling Bedrock 
	- Log the latency when the request gets completed before returning the response of the prompt.

...............................................................................
==========================================================================
PHASE 2 - ADDING INTELLIGENCE
==========================================================================
STEP: RAGv1 (Retrieval -Augmented Generation)
Upgrading the current system from Generic LLM answers to Answers grounded in my knowledge.

RAGv1 Desgin (This is intentionally simple since this is a free-tier account)
- I'm not using any embeddings yet
- I'm not using opensearch or vector db's yet

Instead I'm doing this:
	User Prompt
		|
	Keyword-based retrieval (S3 text files)
		|
	Inject context into prompt
		|
	Claude (Bedrock)
	
With the above approach it would be:
	1. Zero extra AWS Services
	2. Easy to understand
	3. Easy to explain in interviews
	4. Easy to upgrade to embeddings later
	
1. Create the knowledge base locally in the project root "rag_data"
2. In this rag_data, add files like aws_lambda, aws_bedrock, aws_ec2 each explaining about the respective services in 5-20 lines. 
3. Create a bucket in S3 using CLI command: >> aws s3 mb s3://serverless-ai-chatbot-rag
4. Now, move all the contents of the rag_data using command: >> aws s3 sync . s3://serverless-ai-chatbot-rag. THis will upload all the rag_data files in teh S3 bucket.
5. Give lambda permissions to read S3
6. Add S3 retrieval logic.
	- First create S3 client on top in 'app.py'
	- Add a helper function 'load_documents()' to load documents from S3
	- Add another function 'retrieval_context(user_prompt, documents)' for keyword-based retrieval
7. Next Inject context into the prompt. 
I'm enforcing this as my knowldege base so changing my system prompt stating that if the answer is not in the context, say i don't know 

****Learning: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
There are three RAG Behaviors:
1. Augmented (default)
	If RAG context exists -> use it
	If not, answer anyway using LLM Knowledge
	
2. Grounded-Only (Knowledge-base - Enterprise RAG)
	If RAG context exists -> answer
	If not -> Say I don't know
	
3. Strict Domain-restricted bot
	Only answers topics covered by RAG
	Everything else is rejected
	
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FINAL RAG Flow:
1. Load all documents from S2
2. Clean & normalize prompt keywords
3. Score documents correctly
4. if context found -> inject into prompt -> call Bedrock
5. if no context -> return "I don't know" without Bedrock

.........................................................................................
==========================================================================
PHASE 3 - Adding UI
==========================================================================
Frontend Architecture:
Frontend (React) -> API Gateway -> Lambda -> (RAG+Bedrock)

I'll host the frontend on:
S3 static hosting (cheap)
Future work: Cloundfront for better UX + HTTPS + Caching


While doing "npm create vite@latest rag-chat-ui -- --template react":
I selected framework as `react` & variant as 'JavasScript + SWC'

“I focused on system design and cloud integration; the UI was intentionally lightweight.”


Step 1: created a "frontend" folder in the root project : "D:\AWS\Serverless-AI-Powered-Chatbot". 
Step 2: Create Vite React Project: 
>> npm create vite@latest rag-chat-ui -- --template react
>> cd rag-chat-ui
>> npm install

Step 3: Install Material UI
>> npm install @mui/material @emotion/react @emotion/styled
>> npm install @mui/icons-material

Step 4: Create a .env file in frontend/rag-chat-ui/ and the following content in this file:
VITE_API_URL=https://YOUR-API/Prod/hello/
VITE_API_KEY=YOUR-API

Also, go to .gitignore and make sure this exists:
.env
.env.local
.env.*.local

Step 5:
Replace the src/App.jsx code. This will be our UI code 

TO run the server, enter command: 
>> npm run dev

*********************************************************************************************************************************************************************
NOTE: While implementing this i faced this error of CORS!
To resolve this, I had to follow these steps:
1. Went to API Gateway in AWS Console, selected my API (confirmed it is REST API) and selected 'hello' under resources.
2. Click Enable CORS option and see if Default 4XX, Default 5XX, OPTIONS, POST options are selected for Gateway Responses, Access-Control-Allow-Methods. Also, check the value of Access-Control-Allow-Headers : "Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token" and Access-Control-Allow-Origin as '*'.
Click Save and deploy the API with 'prod' as stage.
3. Went to POST under /hello. Clikced on Method Response, Entered HTTP Header as 200 and under Response Header, entered "Access-Control-Allow-Origin" 
4. Hard refresh the Page and check the response the model gives.


But after doing this, I still got the similar error. Then I realized looking at the Integration Response in the POST under /hello that I was using Lambda Proxy Integration, that's why the integration response was locked and I cannot add any headers there.
Therefore, this wasn't a API Gateway problem anymore. I had to fix my Lmabda response because:
 - API Gateway forwards whatever lambda returns 
 - API Gateway does not inject headers
 - Lambda must return all the CORS headers itself
 
Therefore, the main change had to be done in my app.py code in lambda response() handler. "NOTE: This is mandatory when using Lambda proxy integration" 

After changing lambda code, redeploy using sam build & sam deploy, deploy the API once again, hard refresh the UI page and try again with some prompt. It worked!

*********************************************************************************************************************************************************************

Until now, completed:
1. Lambda + Bedrock
2. API Gateway + Throttling
3. RAGv1 (s3-based)
4. Observability
5. Frontend wired 


=====================================================================================
Upgrading the UI
=====================================================================================
1. Create a components folder inside frontend/rag-chat-ui/src 
src/
├── components/
│   ├── ChatMessage.jsx
│   ├── ChatInput.jsx
│   └── ChatHeader.jsx
├── App.jsx
├── main.jsx
└── index.css

-- ChatHeader.jsx:
	Purpose: Clean, reusable top header. This sets visual hierarchy and makes the app feel intentional.
	
-- ChatMessage.jsx: 
	Purpose: 
	Separate logic for:
	 - User messages
	 - Assistant messages
	 - Latency display
	 (A cool feature added is show the messages to be in a loading state where it will show Thinking along with a wheel spinning, later turned it into animation with dots blinking and then load the output response)

-- ChatInput.jsx:
	Purpose:
	Separate logic for:
	 - User messages
	 - Assistant messages
	 - Latency display
	 
2. Modified the App.jsx code and imported these components in this file.

3. For minimal styling appended the styles in index.css. 

4. Add Auto-Scroll Effect. This will take directly to the latest response. THere is no button for auto-scroll but it will directly lead to the latest response

Overall UI incorporates:
- Thinking animation + Spinner
- Smooth Message entrance
- Auto-Scroll
- Input Lock during async 
- Keyboard-native UX
- Auto-resizing input
- Latency feedback
- Clean visual hierarchy


==========================================================================
PHASE 4 - RAG v2 (Smarter Retrieval + Sources)
==========================================================================
Now, time to level up the game of using RAG
This addition will :
- Improve keyword scoring
- Return which documents answered the question.
- Display sources in the UI


NOTE: I will be increasing the tokes to ~150 from a limit of 50-100 since the output seems to be incomplete, answers feel cut, explanations end midsentence etc. 
I will increase them after this RAG v2 because:
RIght now (RAGv1):
1. ReRetrieval is still coarse
2. Context may include irrelevant chunks
3. Longer outputs = higher chance of:
	- verbosity
	- subtle hallucinations
	- wasted tokens

*****************************************	
Architecture:
*****************************************	
________________________________________
										|
User prompt								|
	|									|
Normalize + tokenize					|
	|									|
Score documents intelligently			|
	|									|
Select top-k chunks 					|
	|									|
Inject into system prompt				|
	|									|
Caude Response							|
	|									|
Return response + sources				|
________________________________________|


1. Add a stopword filtering. Words like "what", "is", "the", "how","a","an","explain",etc adds noise and ruin scoring. To stop this from adding added noise, I have added these words as filters and added them in my app.py file.

2. Add sources. This will give the name of the file that was referred during the output response and this will be visible to to the user in the output response. 
For implementing this, I have done changes in: ChatMessage.jsx , App.jsx


Right now, with the current apporoach of the RAG implementation i have done, the problem with this approach is:
I ask -> entire aws_lambda.txt is injected. 
This causes:
1. Bedrock seeing unrelated sentences
2. False keyword matches
3.Sources showing up even when the answer is weak

Correct approach:
Document → split into chunks → score chunks → pass only top chunks


Chunking based retrieval:
- Split documents by paragraphs
- Each chunk ~ 3-6 paragraphs
- Keep source filename with each chunk

In order to make our logic chunk-based, did the changes in load_documents, retrieval_context. 
Also, updated MIN_SCORE as 1 and to boost the source filename, updated the score to 3 from 2.



-------------------------------------------------------------
Final Architecture:

User 
  |
React (Vite)
  | (HTTPS)
API Gateway (REST)
  |
AWS Lambda
  |
Bedrock (Claude)
  |
S3 (RAG Documents)


Logs: cloudwatch
